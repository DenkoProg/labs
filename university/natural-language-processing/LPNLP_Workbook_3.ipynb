{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spellchecker quest\n",
        "\n",
        "–•—Ç–æ—Å—å –Ω–∞—Ä–æ–±–∏–≤ –ø–æ–º–∏–ª–æ–∫ —É –≤—ñ—Ä—à–∞—Ö –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ —Ü—ñ –ø–æ–º–∏–ª–∫–∏ —ñ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "\n",
        "## –ó–∞–¥–∞—á–∞\n",
        "\n",
        "–í–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.train_text`. –¶–µ –∑–≤–∏—á–∞–π–Ω–∏–π –Ω–µ—Ä–æ–∑–º—ñ—á–µ–Ω–∏–π —Ç–µ–∫—Å—Ç. –ù–∞ –Ω—å–æ–º—É –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å. –ü—ñ–¥—ñ–π–¥–µ –±—É–¥—å-—è–∫–∞. –Ø –±–∏ —Ä–∞–¥–∏–≤ feed-forward –Ω–µ–π—Ä–æ–Ω—É –º–æ–¥–µ–ª—å –∑ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—î—é –ø–æ –ª—ñ—Ç–µ—Ä–∞—Ö –∞–±–æ BPE, –±–æ —Ü–µ —Ç–µ, —â–æ –º–∏ –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ª–µ–∫—Ü—ñ—ó. –ê–ª–µ n-–≥—Ä–∞–º–Ω–∞ —Ç–µ–∂ –º–∞—î —Å–ø—Ä–∞—Ü—é–≤–∞—Ç–∏.\n",
        "\n",
        "–¢–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.test_items`. –ü—Ä–∏–∫–ª–∞–¥ –æ–¥–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"text\": \"–°–ø—ñ–≤–∞–ª–∏ –± –ø—Ä–æ–∑—É, —Ç–∞ –ø–æ –Ω–æ–∂–∞—Ö,\",\n",
        "  \"error_start\": 23,\n",
        "  \"error_end\": 28,\n",
        "  \"error\": \"–Ω–æ–∂–∞—Ö\",\n",
        "  \"corrections\": [\n",
        "    \"–Ω–æ–≥–∞—Ö\",\n",
        "    \"–π–æ—Ç–∞—Ö\",\n",
        "    \"—î–Ω–æ—Ç–∞—Ö\",\n",
        "    \"–Ω–æ–∂–∞—Ö\",\n",
        "    \"–Ω–æ—Ç–∞—Ö\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "`error_start` —Ç–∞ `error_end` –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç—ñ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö). –£ –¥–∞–Ω–Ω–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ, –ø–æ–º–∏–ª–∫–æ—é —î `text[23:28]`, —Ç–æ–±—Ç–æ —Å–ª–æ–≤–æ \"–Ω–æ–∂–∞—Ö\".\n",
        "\n",
        "`corrections` -- —Ü–µ —Å–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—å.\n",
        "\n",
        "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å —Ç–∞ –∑–∞ —ó—ó –¥–æ–ø–æ–º–æ–≥–æ—é –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏—Ö.\n",
        "\n",
        "\n",
        "## –ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "\n",
        "–û–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ –≤ `lab.test_items` –¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –æ–¥–Ω—É –ª—ñ—Ç–µ—Ä—É –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –î–ª—è —Ü—å–æ–≥–æ –∑–Ω–∞–π–¥—ñ—Ç—å —Ä—ñ–∑–Ω–∏—Ü—é –º—ñ–∂ –ª—ñ—Ç–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞ –∑ –ø–æ–º–∏–ª–∫–æ—é (`error`) —Ç–∞ –æ–±—Ä–∞–Ω–∏–º –º–æ–¥–µ–ª–ª—é –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º. –ù–∞–¥—Ä—É–∫—É–π—Ç–µ —Ü—é –ª—ñ—Ç–µ—Ä—É. –Ø–∫—â–æ —Å–ª–æ–≤–æ –∑ –ø–æ–º–∏–ª–∫–æ—é –Ω–∞–ø—Ä–∞–≤–¥—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–µ, –∞ —Ç–∞–∫–µ —Ç–µ–∂ –±—É–≤–∞—î, –Ω–∞–¥—Ä—É–∫—É–π—Ç–µ –ø—Ä–æ–±—ñ–ª. –ü—Ä–∏–∫–ª–∞–¥–∏:\n",
        "\n",
        "```\n",
        "Error               Correction     –ù–∞–¥—Ä—É–∫—É–≤–∞—Ç–∏\n",
        "-------------------------------------------\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–ª—ñ—Ç        –ª\n",
        "–ø–Ω—ñ                  –ø–æ–Ω—ñ          –æ\n",
        "–±–∞–ª–ª–µ—Ç               –±–∞–ª–µ—Ç         –ª\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–≤—ñ—Ç        [–ø—Ä–æ–±—ñ–ª]\n",
        "```\n",
        "\n",
        "–ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —è–∫–µ –≤–∏ –ø–æ–±–∞—á–∏—Ç–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ -- —Ü–µ —Ä—è–¥–æ–∫ –∑ –≤—ñ—Ä—à—É –æ–¥–Ω–æ–≥–æ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤.\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∫–≤–µ—Å—Ç -- —ñ–º'—è –∞–≤—Ç–æ—Ä–∞/–∫–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ \"–Ü–º'—è –ü—Ä—ñ–∑–≤–∏—â–µ\".\n",
        "\n",
        "–ü–æ–ª–µ—Ç—ñ–ª–∏! üöÄ"
      ],
      "metadata": {
        "id": "L-Z5-FrvSNPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --ignore-installed https://static.lp-nlp.com/pypi/lpnlp-2025.10.14-py3-none-any.whl"
      ],
      "metadata": {
        "id": "qa35VG1zj2wR",
        "outputId": "792bbd75-9db6-456f-9ebe-1f62344fdd4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/153.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lpnlp\n",
        "\n",
        "lab = lpnlp.start(\n",
        "    email=\"denys.koval.shi.2022@lpnu.ua\",                   # <----------- –ó–∞–ø–æ–≤–Ω—ñ—Ç—å —Ü–µ –ø–æ–ª–µ\n",
        "    lab=\"quest_spellchecker\"\n",
        "    )"
      ],
      "metadata": {
        "id": "DjClbL-Hmcaq",
        "outputId": "0c1cb6be-8dd7-495d-ecbc-2cda7d1e9459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–£–¥–∞—á—ñ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–∫—â–æ –≤–∞–º –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ —É –≤–∏–≥–ª—è–¥—ñ —Ñ–∞–π–ª—É –Ω–∞ –≤–∞—à–æ–º—É –∫–æ–º–ø'—é—Ç–µ—Ä—ñ,\n",
        "# —Ä–æ–∑–∫–æ–º–µ–Ω—Ç—É–π—Ç–µ —Ü–µ–π –∫–æ–¥:\n",
        "\n",
        "# with open(\"train.txt\", \"wt\") as f:\n",
        "#     f.write(lab.train_text)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"train.txt\")"
      ],
      "metadata": {
        "id": "PmFZDgzEorCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ú–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å\n",
        "\n",
        "–ù–∞—Ç—Ä–µ–Ω—É–π—Ç–µ —Å–≤–æ—é –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å —Ç—É—Ç"
      ],
      "metadata": {
        "id": "CkQ69irsm_Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lab.train_text[:330])"
      ],
      "metadata": {
        "id": "DsyqC9fPnMGa",
        "outputId": "1d17c020-efdb-40cc-a09a-de8118ab065e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ôªø–ü–†–ò–ß–ò–ù–ù–ê\r\n",
            "\r\n",
            "–†–µ–≤–µ —Ç–∞ —Å—Ç–æ–≥–Ω–µ –î–Ω—ñ–ø—Ä —à–∏—Ä–æ–∫–∏–π,\r\n",
            "–°–µ—Ä–¥–∏—Ç–∏–π –≤—ñ—Ç–µ—Ä –∑–∞–≤–∏–≤–∞,\r\n",
            "–î–æ–¥–æ–ª—É –≤–µ—Ä–±–∏ –≥–Ω–µ –≤–∏—Å–æ–∫—ñ,\r\n",
            "–ì–æ—Ä–∞–º–∏ —Ö–≤–∏–ª—é –ø—ñ–¥—ñ–π–º–∞.\r\n",
            "–Ü –±–ª—ñ–¥–∏–π –º—ñ—Å—è—Ü—å –Ω–∞ —Ç—É –ø–æ—Ä—É\r\n",
            "–Ü–∑ —Ö–º–∞—Ä–∏ –¥–µ-–¥–µ –≤–∏–≥–ª—è–¥–∞–≤,\r\n",
            "–ù–µ–Ω–∞—á–µ —á–æ–≤–µ–Ω –≤ —Å–∏–Ω—ñ–º –º–æ—Ä—ñ,\r\n",
            "–¢–æ –≤–∏—Ä–∏–Ω–∞–≤, —Ç–æ –ø–æ—Ç–æ–ø–∞–≤.\r\n",
            "–©–µ —Ç—Ä–µ—Ç—ñ –ø—ñ–≤–Ω—ñ –Ω–µ —Å–ø—ñ–≤–∞–ª–∏,\r\n",
            "–ù—ñ—Ö—Ç–æ –Ω—ñ–≥–¥–µ –Ω–µ –≥–æ–º–æ–Ω—ñ–≤,\r\n",
            "–°–∏—á—ñ –≤ –≥–∞—é –ø–µ—Ä–µ–∫–ª–∏–∫–∞–ª–∏—Å—å,\r\n",
            "–¢–∞ —è—Å–µ–Ω —Ä–∞–∑ —É —Ä–∞–∑ —Å–∫—Ä–∏–ø—ñ–≤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "\n",
        "##### –í–∞—à –∫–æ–¥ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç—É—Ç\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
        "CONTEXT_SIZE = 5  # –†–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (—Å–∫—ñ–ª—å–∫–∏ –ª—ñ—Ç–µ—Ä –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è)\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞\n",
        "def build_vocab(text, min_freq=2):\n",
        "    \"\"\"–°—Ç–≤–æ—Ä—é—î —Å–ª–æ–≤–Ω–∏–∫ –ª—ñ—Ç–µ—Ä –∑ —Ç–µ–∫—Å—Ç—É\"\"\"\n",
        "    char_counts = Counter(text)\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "    for char, count in char_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[char] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(lab.train_text, min_freq=1)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"–†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞: {vocab_size}\")\n",
        "\n",
        "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö\n",
        "def create_training_data(text, vocab, context_size):\n",
        "    \"\"\"–°—Ç–≤–æ—Ä—é—î –ø–∞—Ä–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–∞—Å—Ç—É–ø–Ω–∞ –ª—ñ—Ç–µ—Ä–∞) –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\"\"\"\n",
        "    data = []\n",
        "\n",
        "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–∫—Å—Ç –≤ —ñ–Ω–¥–µ–∫—Å–∏\n",
        "    text_indices = [vocab.get(char, vocab['<UNK>']) for char in text]\n",
        "\n",
        "    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ\n",
        "    for i in range(len(text_indices) - context_size):\n",
        "        context = text_indices[i:i + context_size]\n",
        "        target = text_indices[i + context_size]\n",
        "        data.append((context, target))\n",
        "\n",
        "    return data\n",
        "\n",
        "print(\"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö...\")\n",
        "training_data = create_training_data(lab.train_text, vocab, CONTEXT_SIZE)\n",
        "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {len(training_data)}\")\n",
        "\n",
        "# –ú–æ–¥–µ–ª—å\n",
        "class FeedForwardLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(FeedForwardLM, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        out = self.relu(self.linear1(embeds))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear3(out)\n",
        "        return out\n",
        "\n",
        "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}\")\n",
        "\n",
        "model = FeedForwardLM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_SIZE).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
        "print(\"\\n–ü–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ –¥–∞–Ω—ñ\n",
        "    np.random.shuffle(training_data)\n",
        "\n",
        "    # –ë–∞—Ç—á—ñ\n",
        "    for i in range(0, len(training_data), BATCH_SIZE):\n",
        "        batch = training_data[i:i + BATCH_SIZE]\n",
        "\n",
        "        contexts = torch.tensor([item[0] for item in batch], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor([item[1] for item in batch], dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.zero_grad()\n",
        "        outputs = model(contexts)\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (len(training_data) / BATCH_SIZE)\n",
        "    print(f\"–ï–ø–æ—Ö–∞ {epoch + 1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "\n",
        "############################################"
      ],
      "metadata": {
        "id": "tFb93XK8ErR7",
        "outputId": "aa78709c-1d7c-422a-ba68-a576310d9280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞: 118\n",
            "–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö...\n",
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: 414263\n",
            "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: cuda\n",
            "\n",
            "–ü–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...\n",
            "–ï–ø–æ—Ö–∞ 1/10, Loss: 2.2350\n",
            "–ï–ø–æ—Ö–∞ 2/10, Loss: 2.0170\n",
            "–ï–ø–æ—Ö–∞ 3/10, Loss: 1.9542\n",
            "–ï–ø–æ—Ö–∞ 4/10, Loss: 1.9179\n",
            "–ï–ø–æ—Ö–∞ 5/10, Loss: 1.8934\n",
            "–ï–ø–æ—Ö–∞ 6/10, Loss: 1.8762\n",
            "–ï–ø–æ—Ö–∞ 7/10, Loss: 1.8623\n",
            "–ï–ø–æ—Ö–∞ 8/10, Loss: 1.8511\n",
            "–ï–ø–æ—Ö–∞ 9/10, Loss: 1.8409\n",
            "–ï–ø–æ—Ö–∞ 10/10, Loss: 1.8317\n",
            "–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ß–∏—Ç–∞—î–º–æ –º—ñ–∂ —Ä—è–¥–∫—ñ–≤"
      ],
      "metadata": {
        "id": "USBCYglkn_WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import collections\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "# –î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω—Ü—ñ—è:\n",
        "def get_letter(w1: str, w2: str) -> str:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î –ª—ñ—Ç–µ—Ä—É, —è–∫–æ—ó –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è —Å–ª–æ–≤–∞ –∞–±–æ –ø—Ä–æ–±—ñ–ª –¥–ª—è –æ–¥–Ω–∞–∫–æ–≤–∏—Ö —Å–ª—ñ–≤.\n",
        "    \"\"\"\n",
        "\n",
        "    letters1 = collections.Counter(w1)\n",
        "    letters2 = collections.Counter(w2)\n",
        "\n",
        "    diff = letters1 - letters2\n",
        "    if len(diff) != 1:\n",
        "        return \" \"\n",
        "\n",
        "    return diff.most_common()[0][0]\n",
        "\n",
        "\n",
        "def score_text(text: str, model, vocab) -> float:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î log-probability –¥–ª—è —Ç–µ–∫—Å—Ç—É (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ).\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–∫—Å—Ç –≤ —ñ–Ω–¥–µ–∫—Å–∏\n",
        "    text_indices = [vocab.get(char, vocab['<UNK>']) for char in text]\n",
        "\n",
        "    if len(text_indices) <= CONTEXT_SIZE:\n",
        "        return 0.0\n",
        "\n",
        "    total_log_prob = 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(text_indices) - CONTEXT_SIZE):\n",
        "            context = text_indices[i:i + CONTEXT_SIZE]\n",
        "            target = text_indices[i + CONTEXT_SIZE]\n",
        "\n",
        "            context_tensor = torch.tensor([context], dtype=torch.long).to(device)\n",
        "            output = model(context_tensor)\n",
        "\n",
        "            # –û–±—á–∏—Å–ª—é—î–º–æ log probability\n",
        "            log_probs = torch.nn.functional.log_softmax(output, dim=1)\n",
        "            total_log_prob += log_probs[0][target].item()\n",
        "            count += 1\n",
        "\n",
        "    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π log-probability (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ)\n",
        "    return -total_log_prob / count if count > 0 else float('inf')\n",
        "\n",
        "\n",
        "# –ú–æ–∂–µ—Ç–µ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–∞ –≤–µ—Å—å —Ü–µ–π –∫–æ–¥, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n",
        "def solve(model, vocab, test_items) -> Tuple[List[str], str]:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑ —Ç–µ–∫—Å—Ç—ñ–≤ –≤ test_items —Ç–∞\n",
        "    —Å–µ–∫—Ä–µ—Ç–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "    \"\"\"\n",
        "\n",
        "    choices = []\n",
        "    secret = []\n",
        "\n",
        "    for item in test_items:\n",
        "        scores = []\n",
        "        for corr in item['corrections']:\n",
        "\n",
        "            # –ü—ñ–¥—Å—Ç–∞–≤–ª—è—î–º–æ —Å–ª–æ–≤–æ-–∫–∞–Ω–¥–∏–¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç\n",
        "            text = item['text'][:item['error_start']] + corr + item['text'][item['error_end']:]\n",
        "\n",
        "            # –†–∞—Ö—É—î–º–æ score —Ç–µ–∫—Å—Ç—É\n",
        "            score = score_text(text, model, vocab)\n",
        "            scores.append(score)\n",
        "\n",
        "            # print(f'{score:.4f} {text}')\n",
        "\n",
        "        # –°–æ—Ä—Ç—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –Ω–∞ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∑–∞ score\n",
        "        result = sorted(zip(scores, item['corrections']), key=lambda x: x[0])\n",
        "\n",
        "        # –û–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –∑–∞–º—ñ–Ω—É\n",
        "        best = result[0]\n",
        "        best_word = best[1]\n",
        "        choices.append(best_word)\n",
        "\n",
        "        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —á–µ—Ä–≥–æ–≤—É –ª—ñ—Ç–µ—Ä—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "        error = item['error']\n",
        "        letter = get_letter(error, best_word)\n",
        "        secret.append(letter)\n",
        "\n",
        "    secret_message = ''.join(secret)\n",
        "\n",
        "    return choices, secret_message\n",
        "\n",
        "choices, secret_message = solve(model, vocab, lab.test_items)\n",
        "\n",
        "lab.evaluate_accuracy(choices)\n",
        "print(\"SECRET MESSAGE: \", secret_message)\n"
      ],
      "metadata": {
        "id": "suXDePt3pLZ7",
        "outputId": "9194587e-3360-495a-9b67-081f287ddb82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "accuracy = 0.73. –ù–µ–ø–æ–≥–∞–Ω–æ! –°–ø—Ä–æ–±—É–π —Ä–æ–∑–≥–∞–¥–∞–π –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
            "SECRET MESSAGE:   –∏–ù–∑–Ω —î—à —â–æ —Ç–∏—Ä –ª—é–¥ –Ω–∞   –æ–∑–Ω–∞—î—à –ø –æ —Ü–µ —á–∏ –Ω—ñ —É  —ñ  –∞ —Ç–í  –º—ó—î–¥ –Ω–∞ –º—É–∫  —Ç–≤–æ—è  —î–¥–∏–Ω–∞  —á—ñ —Ç–≤–æ—ó  –æ–¥ —ñ –±—ñ–ª—å—à–µ  –µ–±  –Ω–µ –±—É–¥  –∑ –≤—Ç    –∞ —Ü—ñ–π–ì–∑–µ–º–ª  —ñ–Ω—à—ñ  –æ–¥ –¢–∏  —Ç—å –ª –¥–∏ —ñ–ù—à—ñ –∫–æ—Ö–∞—Ç–∏–º—É—Ç—å –ª—é–¥–∏  –¥–æ–±—Ä—ñ  –∞—Å–∫–∞–≤   –º–∑–ª—ñ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.answer(\"–í–∞—Å–∏–ª—å –°–∏–º–æ–Ω–µ–Ω–∫–æ\")   # –ó–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ —ñ–º'—è –∞–≤—Ç–æ—Ä–∞/–∫–∏ —Ä–æ–∑—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤"
      ],
      "metadata": {
        "id": "S7Hmxf6YqUVR",
        "outputId": "934c7ef7-d58e-4edc-eda4-ed9fd496feca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "–ü—Ä–∞–≤–∏–ª—å–Ω–æ! üöÄ –ó–∞–ø–æ–≤–Ω–∏ —Ç–µ–ø–µ—Ä —Ü—é —Ñ–æ—Ä–º—É, –±—É–¥—å –ª–∞—Å–∫–∞: https://tally.so/r/wkl0zZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù  –í—ñ–¥–ø—Ä–∞–≤—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ü–µ–π colab –∞–±–æ PDF –∑ –Ω–∏–º —É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è VNS\n"
      ],
      "metadata": {
        "id": "VpE_-wtdpkHe"
      }
    }
  ]
}